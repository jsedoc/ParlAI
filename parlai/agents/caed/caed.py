#!/usr/bin/env python3

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
"""Example sequence to sequence agent for ParlAI "Creating an Agent" tutorial.
http://parl.ai/static/docs/tutorial_seq2seq.html
"""

from parlai.core.torch_agent import TorchAgent, Output
from .modules import EncoderRNN, DecoderRNN

import torch
from torch import optim
import torch.nn as nn
import torch.nn.functional as F
import spacy, tqdm
import collections
from collections import defaultdict
import concurrent.futures
# from ccg_nlpy import remote_pipeline


class CaedAgent(TorchAgent):
    """Agent which takes an input sequence and produces an output sequence.

    This model is based on Sean Robertson's `seq2seq tutorial
    <http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`_.
    """

    @staticmethod
    def add_cmdline_args(argparser):
        """Add command-line arguments specifically for this agent."""
        TorchAgent.add_cmdline_args(argparser)
        agent = argparser.add_argument_group('Seq2Seq Arguments')
        agent.add_argument('-hs', '--hiddensize', type=int, default=128,
                           help='size of the hidden layers')
        agent.add_argument('-esz', '--embeddingsize', type=int, default=128,
                           help='size of the token embeddings')
        agent.add_argument('-nl', '--numlayers', type=int, default=2,
                           help='number of hidden layers')
        agent.add_argument('-lr', '--learningrate', type=float, default=1,
                           help='learning rate')
        agent.add_argument('-dr', '--dropout', type=float, default=0.1,
                           help='dropout rate')
        agent.add_argument('--gpu', type=int, default=-1,
                           help='which GPU device to use')
        agent.add_argument('-rf', '--report-freq', type=float, default=0.001,
                           help='Report frequency of prediction during eval.')
        CaedAgent.dictionary_class().add_cmdline_args(argparser)
        return agent

    def __init__(self, opt, shared=None):
        """Initialize example seq2seq agent.

        :param opt: options dict generated by parlai.core.params:ParlaiParser
        :param shared: optional shared dict with preinitialized model params
        """
        super().__init__(opt, shared)

        self.id = 'Seq2Seq'
        self.nlp = spacy.load('en_core_web_lg')

        if not shared:
            # set up model from scratch
            hsz = opt['hiddensize']
            nl = opt['numlayers']

            # encoder captures the input text
            self.encoder = EncoderRNN(len(self.dict), hsz, nl)
            # decoder produces our output states
            self.decoder = DecoderRNN(len(self.dict), hsz, nl)

            if self.use_cuda:  # set in parent class
                self.encoder.cuda()
                self.decoder.cuda()

            if opt.get('numthreads', 1) > 1:
                self.encoder.share_memory()
                self.decoder.share_memory()
        elif 'encoder' in shared:
            # copy initialized data from shared table
            self.encoder = shared['encoder']
            self.decoder = shared['decoder']

        # set up the criterion
        self.criterion = nn.NLLLoss()

        # set up optims for each module
        lr = opt['learningrate']
        self.optims = {
            'encoder': optim.SGD(self.encoder.parameters(), lr=lr),
            'decoder': optim.SGD(self.decoder.parameters(), lr=lr),
        }

        self.longest_label = 1
        self.hiddensize = opt['hiddensize']
        self.numlayers = opt['numlayers']
        self.START = torch.LongTensor([self.START_IDX])
        if self.use_cuda:
            self.START = self.START.cuda()

        self.reset()

    def zero_grad(self):
        """Zero out optimizer."""
        for optimizer in self.optims.values():
            optimizer.zero_grad()

    def update_params(self):
        """Do one optimization step."""
        for optimizer in self.optims.values():
            optimizer.step()

    def share(self):
        """Share internal states."""
        shared = super().share()
        shared['encoder'] = self.encoder
        shared['decoder'] = self.decoder
        return shared

    # only works if text is of the form "Can I see your AMA ?" with a space before punctuation
    # def process_line(self, text):
    #     target_ents = ["ORG", "PERSON"]
    #     print('\n')

    #     print('ORIGINAL TEXT :' + text + '\n')

    #     l = text

    #     word_to_ent = dict()
    #     ent_counts = dict()

    #     spdoc = self.nlp(l)
                    
    #     for e in spdoc.ents:
    #         word_to_ent[e.as_doc().text.strip()] = e.label_

    #     for word in word_to_ent:
    #         #print(word + "..." + word_to_ent[word])
    #         ent = word_to_ent[word]
    #         previous_count = 0
    #         if ent in ent_counts:
    #             previous_count = ent_counts[ent]
    #         ent_counts[ent] = previous_count + 1
    #         word_to_ent[word] = word_to_ent[word] + '@' + str(ent_counts[ent])
            
    #     for word in word_to_ent:
    #       if (word != ''):
    #         text = text.replace(word, word_to_ent[word])
        
    #     print('CONVERTED TEXT :' + text + '\n')
    #     return text

    def process_line(self, text):
    
        # first preprocess using database of movie labels
      
        target_ents = ["ORG", "PERSON"]

        l = text

        if "__null__" in text:
        	return ""

        print('\n')
        print('ORIGINAL TEXT :' + text + '\n')

        word_to_ent = dict()
        ent_counts = dict()
        movie_labels = ['PER']
        
        # movie ents
        # p = remote_pipeline.RemotePipeline()

        # doc = p.doc(text)
        
        # if doc != None:
        #   ner_view = doc.get_ner_conll
        #   if ner_view.get_cons() != None:
        #     for ner in ner_view:
        #       print(ner['label'])
        #       if ner['label'] in movie_labels:
        #         word_to_ent[ner['tokens']] = ner['label']
        
        # end of movie ents

        spdoc = self.nlp(l)
                    
        for e in spdoc.ents:
            #  word -> entity
            word = e.as_doc().text.strip()
            if word not in word_to_ent:
              word_to_ent[word] = e.label_

        for word in word_to_ent:
            ent = word_to_ent[word]
            previous_count = 0
            if ent in ent_counts:
                previous_count = ent_counts[ent]
            ent_counts[ent] = previous_count + 1
            word_to_ent[word] = word_to_ent[word] + '@' + str(ent_counts[ent])
            
        for word in word_to_ent:
          if (word != ''):
            text = text.replace(word, word_to_ent[word])
          
        print('CONVERTED TEXT :' + text + '\n')
        return text

    def get_tmdb_ids(self):
    	data = json.loads('http://files.tmdb.org/p/exports/movie_ids_04_28_2017.json.gz')
        

    def v2t(self, vector):
        """Convert vector to text.

        :param vector: tensor of token indices.
            1-d tensors will return a string, 2-d will return a list of strings
        """
        if vector.dim() == 1:
            output_tokens = []
            # Remove the final END_TOKEN that is appended to predictions
            for token in vector:
                if token == self.END_IDX:
                    break
                else:
                    output_tokens.append(token)
            return self.process_line(self.dict.vec2txt(output_tokens))
        elif vector.dim() == 2:
            return [self.v2t(vector[i]) for i in range(vector.size(0))]
        raise RuntimeError('Improper input to v2t with dimensions {}'.format(
            vector.size()))

    # we can change vectorize to use spacey
    def vectorize(self, *args, **kwargs):
        """Call vectorize without adding start tokens to labels."""
        kwargs['add_start'] = False

        # (Pdb) p args[0]['text'] 
        # 'your persona: i like to remodel homes.\nyour persona: i like to go hunting.\nyour persona: i like to shoot a bow.\nyour persona: my favorite holiday is halloween.\nhi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .'
        # (Pdb) p args[0]['labels']
        # ('you must be very fast . hunting is one of my favorite hobbies .',)
        args[0]['text'] = self.process_line(args[0]['text'])

        return super().vectorize(*args, **kwargs)

    def train_step(self, batch):
        """Train model to produce ys given xs.

        :param batch: parlai.core.torch_agent.Batch, contains tensorized
                      version of observations.

        Return estimated responses, with teacher forcing on the input sequence
        (list of strings of length batchsize).
        """
        xs, ys = batch.text_vec, batch.label_vec
        if xs is None:
            return
        bsz = xs.size(0)
        starts = self.START.expand(bsz, 1)  # expand to batch size
        loss = 0
        self.zero_grad()
        self.encoder.train()
        self.decoder.train()
        target_length = ys.size(1)
        # save largest seen label for later
        self.longest_label = max(target_length, self.longest_label)

        _encoder_output, encoder_hidden = self.encoder(xs)

        # Teacher forcing: Feed the target as the next input
        y_in = ys.narrow(1, 0, ys.size(1) - 1)
        decoder_input = torch.cat([starts, y_in], 1)
        decoder_output, decoder_hidden = self.decoder(decoder_input,
                                                      encoder_hidden)

        scores = decoder_output.view(-1, decoder_output.size(-1))
        loss = self.criterion(scores, ys.view(-1))
        loss.backward()
        self.update_params()

        _max_score, predictions = decoder_output.max(2)
        return Output(self.v2t(predictions))

    def eval_step(self, batch):
        """Generate a response to the input tokens.

        :param batch: parlai.core.torch_agent.Batch, contains tensorized
                      version of observations.

        Return predicted responses (list of strings of length batchsize).
        """
        xs = batch.text_vec
        if xs is None:
            return
        bsz = xs.size(0)
        starts = self.START.expand(bsz, 1)  # expand to batch size
        # just predict
        self.encoder.eval()
        self.decoder.eval()
        _encoder_output, encoder_hidden = self.encoder(xs)

        predictions = []
        done = [False for _ in range(bsz)]
        total_done = 0
        decoder_input = starts
        decoder_hidden = encoder_hidden

        for _ in range(self.longest_label):
            # generate at most longest_label tokens
            decoder_output, decoder_hidden = self.decoder(decoder_input,
                                                          decoder_hidden)
            _max_score, preds = decoder_output.max(2)
            predictions.append(preds)
            decoder_input = preds  # set input to next step

            # check if we've produced the end token
            for b in range(bsz):
                if not done[b]:
                    # only add more tokens for examples that aren't done
                    if preds[b].item() == self.END_IDX:
                        # if we produced END, we're done
                        done[b] = True
                        total_done += 1
            if total_done == bsz:
                # no need to generate any more
                break
        predictions = torch.cat(predictions, 1)
        return Output(self.v2t(predictions))
